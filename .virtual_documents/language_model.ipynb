import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt


#documents loading
corpus = ["earth jupyter planet", "jupyter earth planet", "planet earth jupyter",
          "orange strawberry fruit", "strawberry orange fruit", "fruit orange strawberry"]


# tokenization
tokenized_corpus = [sent.split(" ") for sent in corpus]
print(tokenized_corpus)


# numeralization
flatten = lambda l: [item for sublist in l for item in sublist]
flatten(tokenized_corpus)
unique_words = list(set(flatten(tokenized_corpus)))
print(unique_words)


#mapping between words and index
word2index= {v:idx for idx, v in enumerate(unique_words)}
word2index['jupyter']


unique_words.append('<UNK>')
word2index['<UNK>'] = 6


#mapping index to words
index2word = {v:k for k,v in word2index.items()}
index2word


#preparing data 
import numpy as np

def random_batch(batch_size, corpus):
    skipgrams = []
    
    #loop each corpus
    for doc in tokenized_corpus:
        #loop each document
        for i in range(1, len(doc)-1):
            #center word
            center = word2index[doc[i]]
            #outside words 2
            outside_words = (word2index[doc[i-1]], word2index[doc[i+1]])
            #we append each of the outside words to a list
            for each_out in outside_words:
                skipgrams.append([center, each_out])
                #center1->outside1; center2->outside2
    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)
    inputs, labels = [],[]
    for index in random_index:
        inputs.append([skipgrams[index][0]])
        labels.append([skipgrams[index][1]])

    return np.array(inputs), np.array(labels)
x,y = random_batch(2,tokenized_corpus)


embedding = nn.Embedding(7,2)


x_tensor = torch.LongTensor(x)
embedding(x_tensor).shape #(batch_size,1,emb_size)


#now we will focus on creating the model
class Skipgram(nn.Module):
    def __init__(self, voc_size, emb_size):
        super(Skipgram, self).__init__()
        self.embedding_center = nn.Embedding(voc_size, emb_size)
        self.embedding_outside = nn.Embedding(voc_size, emb_size)
    
    def forward(self, center, outside, all_vocabs):
        center_vector = self.embedding_center(center)  # (batch_size, 1, emb_size)
        outside_vector = self.embedding_outside(outside)  # (batch_size, 1, emb_size)
        vocabulary_vector = self.embedding_center(all_vocabs)  # (batch_size, vocabulary_size, emb_size)
        
        top_term = torch.exp(torch.bmm(outside_vector, center_vector.transpose(1, 2)).squeeze(2))
        # (batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)
        
        lower_term = torch.bmm(vocabulary_vector, center_vector.transpose(1, 2)).squeeze(2)
        # (batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)
        
        lower_term_sum = torch.sum(torch.exp(lower_term), dim=1)  # (batch_size, 1)
        
        loss_function = -torch.mean(torch.log(top_term / lower_term_sum))
        return loss_function


#preparing all vocabs
batch_size = 2
voc_size   = len(unique_words)
def prepare_sequence(seq, word2index):
    indexes = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index["<UNK>"], seq))
    return torch.LongTensor(indexes)
all_vocabs = prepare_sequence(list(unique_words), word2index).expand(batch_size, voc_size)
all_vocabs


model = Skipgram(voc_size, 2)
model


#turning dimensions x and y into pytorch tensors
input_tensor = torch.LongTensor(x)
label_tensor = torch.LongTensor(y)


loss = model(input_tensor, label_tensor, all_vocabs)



loss
#whith each entire code re execution, we get a lower the loss value, and this is what we want, to minimise the loss function output


#now let's dive in to training the data

import torch.optim as optim



batch_size = 2
emb_size = 2
model = Skipgram(voc_size,emb_size)
optimizer = optim.Adam(model.parameters(), lr=0.001)


num_epochs = 5000
for epoch in range(num_epochs):

    #get batch
    input_batch, label_batch =  random_batch(batch_size, tokenized_corpus)
    input_tensor = torch.LongTensor(input_batch)
    label_tensor = torch.LongTensor(label_batch)
    #predict
    loss = model(input_tensor, label_tensor, all_vocabs)
    #backpropegate
    optimizer.zero_grad()
    loss.backward()
    #update aplpha
    optimizer.step()
    #print the loss
    if(epoch+1)%1000 ==0:
        print(f"Epoch{epoch+1:6.0f}  \\ Loss:{loss:2.6f}")
        


# now let's dive in to Plot the embeddings(data visualisation using MatPlotLib
jupyter = torch.LongTensor([word2index['jupyter']])
jupyter


jup_embed_c = model.embedding_center(jupyter)
jup_embed_o = model.embedding_outside(jupyter)
jup_embed = (jup_embed_c+jup_embed_o)/2
jup_embed


def get_embed(word):
    try:
        index = word2index[word]
    except:
        index = word2index['<UNK>']
        
    word = torch.LongTensor([word2index[word]])
    
    embed_c = model.embedding_center(word)
    embed_o = model.embedding_outside(word)
    embed   = (embed_c + embed_o) / 2
    
    return embed[0][0].item(), embed[0][1].item()


get_embed('strawberry')


get_embed('planet')


get_embed('orange')


get_embed('jupyter')


get_embed('earth')


get_embed('planet')


plt.figure(figsize=(6, 3))
for i, word in enumerate(unique_words):
    x, y = get_embed(word)
    plt.scatter(x, y)
    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')
plt.show()


earth = get_embed('earth')
earth


unk = get_embed('<UNK>')
unk


np.array(earth) @ np.array(unk)


planet = get_embed('planet')


def cosine_similarity(A, B):
    dot_product = np.dot(A, B)
    norm_a = np.linalg.norm(A)
    norm_b = np.linalg.norm(B)
    similarity = dot_product / (norm_a * norm_b)
    return similarity

print(cosine_similarity(np.array(earth), np.array(unk)))
print(cosine_similarity(np.array(earth), np.array(planet)))
